{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assigning `super_book_codes` to the banned books\n",
    "\n",
    "**Author:** Michael Falk\n",
    "\n",
    "**Date:** 31/10/18-1/11/18, 6/11/18, 12/11/18\n",
    "\n",
    "## Background\n",
    "\n",
    "One of the key datasets for *Mapping Print, Charting Enlightenment* is a set of documents concerning illegal books in eighteenth-century France. BNF MS 21928-9 contains a list of banned books. It is unclear who exactly wrote the list, but it appears to have been prepared by the central government to assist book inspectors with their tasks across France. BNF Arsenal MS 10305 is an inventory of all the books that were found in the Bastille when it was stormed during the French Revolution. The actual MS has disappeared, but luckily a modern edition exists.\n",
    "\n",
    "A problem occured during entry of the data. The interface was supposed to oblige the user to assign a 'super book code' to each title in the banned books lists upon entry. But due to a glitch in the interface, the lookup took too long at it was impossible to efficiently do so. Accordingly, only 97 of the 1000+ illegal books have a 'super book code' assigned to them. The data is therefore not linked to the rest of the database and is useless for analysis.\n",
    "\n",
    "To speed up the process of linking all the data, this notebook uses 'dedupe', an open-source record linkage library, to try and find links between these banned titles and the titles already recorded elsewhere in the database. Hopefully this will speed up record linkage, and provide a testbed for other record linkage tasks in the project.\n",
    "\n",
    "**Update:** Better versions of the helper functions defined in this notebook have been saved in the file *dedupe_helper_functions.py* in this repo.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Import data, initialise model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1.1: Load necessary libraries and define key paths\n",
    "import dedupe as dd\n",
    "import pandas as pd\n",
    "import os as os\n",
    "import time\n",
    "import numpy as np\n",
    "import random\n",
    "from dedupe_helper_functions import dedupe_initialise, run_deduper, save_clusters\n",
    "import json\n",
    "\n",
    "input_file = \"combined_editions_illegal_books.csv\"\n",
    "output_file = \"illegal_books_deduped.csv\"\n",
    "settings_file = \"illegal_books_learned_settings\"\n",
    "training_file = \"illegal_books_training.json\"\n",
    "output_file = \"illegal_books_clustered.csv\"\n",
    "marked_pairs_file = \"marked_pairs.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data was preprocessed in R. The full set of 'editions' was extracted from the database. The illegal_titles data was cleaned, and the two datasets were combined into a single large table. The script is in this repo. This preprocessing means that the problem is now a problem of finding duplicate rows in a single table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data has 17164 rows, 1921 of which need super_book_codes assigned.\n"
     ]
    }
   ],
   "source": [
    "# Cell 1.2: Import data\n",
    "data_frame = pd.read_csv(input_file)\n",
    "\n",
    "print(f\"The data has {data_frame.shape[0]} rows, {data_frame[data_frame['super_book_code'].isna()].shape[0]} of which need super_book_codes assigned.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:dedupe.api:(TfidfTextCanopyPredicate: (0.2, full_book_title), (SimplePredicate: (firstTokenPredicate, author_name), SimplePredicate: (sameSevenCharStartPredicate, author_name)), TfidfTextCanopyPredicate: (0.4, full_book_title), (SimplePredicate: (sameSevenCharStartPredicate, full_book_title), TfidfNGramCanopyPredicate: (0.2, full_book_title)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading pre-trained model from illegal_books_learned_settings...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Cell 1.3: Initialise deduper\n",
    "\n",
    "# Creat a list of fields for the model to look at. NB: 'ID' and 'UUID' are not relevant to the task,\n",
    "# hence do not appear in the list. The 'super_book_code' also encodes no useful information,\n",
    "# because the problem is that we have records without codes, and the model will learn to focus on that\n",
    "# column too much if we include it, since it is a nearly perfect determinant of identity.\n",
    "fields = [\n",
    "    {'field':'full_book_title', 'type': 'String'},\n",
    "    {'field':'author_name', 'type': 'String'},\n",
    "    {'field':'stated_publication_places', 'type': 'String'},\n",
    "    {'field':'stated_publication_years', 'type': 'DateTime'}\n",
    "]\n",
    "\n",
    "# This line creates the Dedupe model. If it finds a file at the 'settings_file', it will load as a 'static'\n",
    "# deduper that can be used efficiently to cluster data, but that cannot be trained. If there is no\n",
    "# settings file at the given path, it will initialise as an active deduper that must be trained before it can be used.\n",
    "deduper = dedupe_initialise(data_frame, fields, settings_file, training_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2.1a: Adding training data to the model (console)\n",
    "\n",
    "# Run this cell to open the console labeller, which allows you to manually enter training data in the output window.\n",
    "dd.consoleLabel(deduper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'StaticDedupe' object has no attribute 'markPairs'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-9e806d41a320>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mmarked_pairs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mdeduper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmarkPairs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmarked_pairs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'StaticDedupe' object has no attribute 'markPairs'"
     ]
    }
   ],
   "source": [
    "# Cell 2.1b: Adding training data to the model (marked pairs json file)\n",
    "\n",
    "# Run this cell to import the training json file generated by 'illegal_books_get_marked_pairs.R', and add it to the model.\n",
    "with open(marked_pairs_file, 'r') as f:\n",
    "    marked_pairs = json.load(f)\n",
    "\n",
    "deduper.markPairs(marked_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing threshold based on a recall weighting of 0.5.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:dedupe.canopy_index:Removing stop word de\n",
      "INFO:dedupe.canopy_index:Removing stop word les\n",
      "INFO:dedupe.canopy_index:Removing stop word ou\n",
      "INFO:dedupe.canopy_index:Removing stop word M\n",
      "INFO:dedupe.canopy_index:Removing stop word sur\n",
      "INFO:dedupe.canopy_index:Removing stop word et\n",
      "INFO:dedupe.canopy_index:Removing stop word la\n",
      "INFO:dedupe.canopy_index:Removing stop word par\n",
      "INFO:dedupe.canopy_index:Removing stop word du\n",
      "INFO:dedupe.canopy_index:Removing stop word à\n",
      "INFO:dedupe.canopy_index:Removing stop word des\n",
      "INFO:dedupe.canopy_index:Removing stop word le\n",
      "INFO:dedupe.canopy_index:Removing stop word en\n",
      "INFO:dedupe.canopy_index:Removing stop word  D\n",
      "INFO:dedupe.canopy_index:Removing stop word  d\n",
      "INFO:dedupe.canopy_index:Removing stop word ad\n",
      "INFO:dedupe.canopy_index:Removing stop word ch\n",
      "INFO:dedupe.canopy_index:Removing stop word de\n",
      "INFO:dedupe.canopy_index:Removing stop word es\n",
      "INFO:dedupe.canopy_index:Removing stop word ie\n",
      "INFO:dedupe.canopy_index:Removing stop word li\n",
      "INFO:dedupe.canopy_index:Removing stop word me\n",
      "INFO:dedupe.canopy_index:Removing stop word ou\n",
      "INFO:dedupe.canopy_index:Removing stop word s \n",
      "INFO:dedupe.canopy_index:Removing stop word si\n",
      "INFO:dedupe.canopy_index:Removing stop word uv\n",
      "INFO:dedupe.canopy_index:Removing stop word en\n",
      "INFO:dedupe.canopy_index:Removing stop word se\n",
      "INFO:dedupe.canopy_index:Removing stop word  v\n",
      "INFO:dedupe.canopy_index:Removing stop word er\n",
      "INFO:dedupe.canopy_index:Removing stop word or\n",
      "INFO:dedupe.canopy_index:Removing stop word  C\n",
      "INFO:dedupe.canopy_index:Removing stop word Co\n",
      "INFO:dedupe.canopy_index:Removing stop word he\n",
      "INFO:dedupe.canopy_index:Removing stop word le\n",
      "INFO:dedupe.canopy_index:Removing stop word  L\n",
      "INFO:dedupe.canopy_index:Removing stop word  a\n",
      "INFO:dedupe.canopy_index:Removing stop word  l\n",
      "INFO:dedupe.canopy_index:Removing stop word  p\n",
      "INFO:dedupe.canopy_index:Removing stop word  s\n",
      "INFO:dedupe.canopy_index:Removing stop word Le\n",
      "INFO:dedupe.canopy_index:Removing stop word ag\n",
      "INFO:dedupe.canopy_index:Removing stop word an\n",
      "INFO:dedupe.canopy_index:Removing stop word at\n",
      "INFO:dedupe.canopy_index:Removing stop word ge\n",
      "INFO:dedupe.canopy_index:Removing stop word in\n",
      "INFO:dedupe.canopy_index:Removing stop word it\n",
      "INFO:dedupe.canopy_index:Removing stop word nn\n",
      "INFO:dedupe.canopy_index:Removing stop word nt\n",
      "INFO:dedupe.canopy_index:Removing stop word on\n",
      "INFO:dedupe.canopy_index:Removing stop word qu\n",
      "INFO:dedupe.canopy_index:Removing stop word ra\n",
      "INFO:dedupe.canopy_index:Removing stop word t \n",
      "INFO:dedupe.canopy_index:Removing stop word tt\n",
      "INFO:dedupe.canopy_index:Removing stop word té\n",
      "INFO:dedupe.canopy_index:Removing stop word ui\n",
      "INFO:dedupe.canopy_index:Removing stop word ée\n",
      "INFO:dedupe.canopy_index:Removing stop word  B\n",
      "INFO:dedupe.canopy_index:Removing stop word ci\n",
      "INFO:dedupe.canopy_index:Removing stop word  P\n",
      "INFO:dedupe.canopy_index:Removing stop word du\n",
      "INFO:dedupe.canopy_index:Removing stop word ss\n",
      "INFO:dedupe.canopy_index:Removing stop word a \n",
      "INFO:dedupe.canopy_index:Removing stop word ic\n",
      "INFO:dedupe.canopy_index:Removing stop word la\n",
      "INFO:dedupe.canopy_index:Removing stop word mo\n",
      "INFO:dedupe.canopy_index:Removing stop word po\n",
      "INFO:dedupe.canopy_index:Removing stop word  R\n",
      "INFO:dedupe.canopy_index:Removing stop word em\n",
      "INFO:dedupe.canopy_index:Removing stop word ri\n",
      "INFO:dedupe.canopy_index:Removing stop word ab\n",
      "INFO:dedupe.canopy_index:Removing stop word vi\n",
      "INFO:dedupe.canopy_index:Removing stop word ac\n",
      "INFO:dedupe.canopy_index:Removing stop word ce\n",
      "INFO:dedupe.canopy_index:Removing stop word el\n",
      "INFO:dedupe.canopy_index:Removing stop word hi\n",
      "INFO:dedupe.canopy_index:Removing stop word iv\n",
      "INFO:dedupe.canopy_index:Removing stop word ni\n",
      "INFO:dedupe.canopy_index:Removing stop word os\n",
      "INFO:dedupe.canopy_index:Removing stop word ph\n",
      "INFO:dedupe.canopy_index:Removing stop word ré\n",
      "INFO:dedupe.canopy_index:Removing stop word un\n",
      "INFO:dedupe.canopy_index:Removing stop word al\n",
      "INFO:dedupe.canopy_index:Removing stop word ct\n",
      "INFO:dedupe.canopy_index:Removing stop word ec\n",
      "INFO:dedupe.canopy_index:Removing stop word nd\n",
      "INFO:dedupe.canopy_index:Removing stop word ta\n",
      "INFO:dedupe.canopy_index:Removing stop word co\n",
      "INFO:dedupe.canopy_index:Removing stop word io\n",
      "INFO:dedupe.canopy_index:Removing stop word pr\n",
      "INFO:dedupe.canopy_index:Removing stop word st\n",
      "INFO:dedupe.canopy_index:Removing stop word ut\n",
      "INFO:dedupe.canopy_index:Removing stop word  r\n",
      "INFO:dedupe.canopy_index:Removing stop word to\n",
      "INFO:dedupe.canopy_index:Removing stop word so\n",
      "INFO:dedupe.canopy_index:Removing stop word mi\n",
      "INFO:dedupe.canopy_index:Removing stop word  t\n",
      "INFO:dedupe.canopy_index:Removing stop word au\n",
      "INFO:dedupe.canopy_index:Removing stop word ux\n",
      "INFO:dedupe.canopy_index:Removing stop word à \n",
      "INFO:dedupe.canopy_index:Removing stop word mp\n",
      "INFO:dedupe.canopy_index:Removing stop word  g\n",
      "INFO:dedupe.canopy_index:Removing stop word ts\n",
      "INFO:dedupe.canopy_index:Removing stop word n \n",
      "INFO:dedupe.canopy_index:Removing stop word av\n",
      "INFO:dedupe.canopy_index:Removing stop word ed\n",
      "INFO:dedupe.canopy_index:Removing stop word ig\n",
      "INFO:dedupe.canopy_index:Removing stop word ng\n",
      "INFO:dedupe.canopy_index:Removing stop word gi\n",
      "INFO:dedupe.canopy_index:Removing stop word é \n",
      "INFO:dedupe.canopy_index:Removing stop word no\n",
      "INFO:dedupe.canopy_index:Removing stop word rt\n",
      "INFO:dedupe.blocking:10000, 3.0141442 seconds\n",
      "INFO:dedupe.api:Maximum expected recall and precision\n",
      "INFO:dedupe.api:recall: 0.519\n",
      "INFO:dedupe.api:precision: 0.817\n",
      "INFO:dedupe.api:With threshold: 0.588\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computation complete. Threshold = 0.5879074931144714. It took 60.914 seconds.\n",
      "Clustering...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:dedupe.canopy_index:Removing stop word de\n",
      "INFO:dedupe.canopy_index:Removing stop word les\n",
      "INFO:dedupe.canopy_index:Removing stop word ou\n",
      "INFO:dedupe.canopy_index:Removing stop word M\n",
      "INFO:dedupe.canopy_index:Removing stop word sur\n",
      "INFO:dedupe.canopy_index:Removing stop word et\n",
      "INFO:dedupe.canopy_index:Removing stop word la\n",
      "INFO:dedupe.canopy_index:Removing stop word par\n",
      "INFO:dedupe.canopy_index:Removing stop word du\n",
      "INFO:dedupe.canopy_index:Removing stop word à\n",
      "INFO:dedupe.canopy_index:Removing stop word des\n",
      "INFO:dedupe.canopy_index:Removing stop word le\n",
      "INFO:dedupe.canopy_index:Removing stop word en\n",
      "INFO:dedupe.canopy_index:Removing stop word  D\n",
      "INFO:dedupe.canopy_index:Removing stop word  d\n",
      "INFO:dedupe.canopy_index:Removing stop word ad\n",
      "INFO:dedupe.canopy_index:Removing stop word ch\n",
      "INFO:dedupe.canopy_index:Removing stop word de\n",
      "INFO:dedupe.canopy_index:Removing stop word es\n",
      "INFO:dedupe.canopy_index:Removing stop word ie\n",
      "INFO:dedupe.canopy_index:Removing stop word li\n",
      "INFO:dedupe.canopy_index:Removing stop word me\n",
      "INFO:dedupe.canopy_index:Removing stop word ou\n",
      "INFO:dedupe.canopy_index:Removing stop word s \n",
      "INFO:dedupe.canopy_index:Removing stop word si\n",
      "INFO:dedupe.canopy_index:Removing stop word uv\n",
      "INFO:dedupe.canopy_index:Removing stop word en\n",
      "INFO:dedupe.canopy_index:Removing stop word se\n",
      "INFO:dedupe.canopy_index:Removing stop word  v\n",
      "INFO:dedupe.canopy_index:Removing stop word er\n",
      "INFO:dedupe.canopy_index:Removing stop word or\n",
      "INFO:dedupe.canopy_index:Removing stop word  C\n",
      "INFO:dedupe.canopy_index:Removing stop word Co\n",
      "INFO:dedupe.canopy_index:Removing stop word he\n",
      "INFO:dedupe.canopy_index:Removing stop word le\n",
      "INFO:dedupe.canopy_index:Removing stop word  L\n",
      "INFO:dedupe.canopy_index:Removing stop word  a\n",
      "INFO:dedupe.canopy_index:Removing stop word  l\n",
      "INFO:dedupe.canopy_index:Removing stop word  p\n",
      "INFO:dedupe.canopy_index:Removing stop word  s\n",
      "INFO:dedupe.canopy_index:Removing stop word Le\n",
      "INFO:dedupe.canopy_index:Removing stop word ag\n",
      "INFO:dedupe.canopy_index:Removing stop word an\n",
      "INFO:dedupe.canopy_index:Removing stop word at\n",
      "INFO:dedupe.canopy_index:Removing stop word ge\n",
      "INFO:dedupe.canopy_index:Removing stop word in\n",
      "INFO:dedupe.canopy_index:Removing stop word it\n",
      "INFO:dedupe.canopy_index:Removing stop word nn\n",
      "INFO:dedupe.canopy_index:Removing stop word nt\n",
      "INFO:dedupe.canopy_index:Removing stop word on\n",
      "INFO:dedupe.canopy_index:Removing stop word qu\n",
      "INFO:dedupe.canopy_index:Removing stop word ra\n",
      "INFO:dedupe.canopy_index:Removing stop word t \n",
      "INFO:dedupe.canopy_index:Removing stop word tt\n",
      "INFO:dedupe.canopy_index:Removing stop word té\n",
      "INFO:dedupe.canopy_index:Removing stop word ui\n",
      "INFO:dedupe.canopy_index:Removing stop word ée\n",
      "INFO:dedupe.canopy_index:Removing stop word  B\n",
      "INFO:dedupe.canopy_index:Removing stop word ci\n",
      "INFO:dedupe.canopy_index:Removing stop word  P\n",
      "INFO:dedupe.canopy_index:Removing stop word du\n",
      "INFO:dedupe.canopy_index:Removing stop word ss\n",
      "INFO:dedupe.canopy_index:Removing stop word a \n",
      "INFO:dedupe.canopy_index:Removing stop word ic\n",
      "INFO:dedupe.canopy_index:Removing stop word la\n",
      "INFO:dedupe.canopy_index:Removing stop word mo\n",
      "INFO:dedupe.canopy_index:Removing stop word po\n",
      "INFO:dedupe.canopy_index:Removing stop word  R\n",
      "INFO:dedupe.canopy_index:Removing stop word em\n",
      "INFO:dedupe.canopy_index:Removing stop word ri\n",
      "INFO:dedupe.canopy_index:Removing stop word ab\n",
      "INFO:dedupe.canopy_index:Removing stop word vi\n",
      "INFO:dedupe.canopy_index:Removing stop word ac\n",
      "INFO:dedupe.canopy_index:Removing stop word ce\n",
      "INFO:dedupe.canopy_index:Removing stop word el\n",
      "INFO:dedupe.canopy_index:Removing stop word hi\n",
      "INFO:dedupe.canopy_index:Removing stop word iv\n",
      "INFO:dedupe.canopy_index:Removing stop word ni\n",
      "INFO:dedupe.canopy_index:Removing stop word os\n",
      "INFO:dedupe.canopy_index:Removing stop word ph\n",
      "INFO:dedupe.canopy_index:Removing stop word ré\n",
      "INFO:dedupe.canopy_index:Removing stop word un\n",
      "INFO:dedupe.canopy_index:Removing stop word al\n",
      "INFO:dedupe.canopy_index:Removing stop word ct\n",
      "INFO:dedupe.canopy_index:Removing stop word ec\n",
      "INFO:dedupe.canopy_index:Removing stop word nd\n",
      "INFO:dedupe.canopy_index:Removing stop word ta\n",
      "INFO:dedupe.canopy_index:Removing stop word co\n",
      "INFO:dedupe.canopy_index:Removing stop word io\n",
      "INFO:dedupe.canopy_index:Removing stop word pr\n",
      "INFO:dedupe.canopy_index:Removing stop word st\n",
      "INFO:dedupe.canopy_index:Removing stop word ut\n",
      "INFO:dedupe.canopy_index:Removing stop word  r\n",
      "INFO:dedupe.canopy_index:Removing stop word to\n",
      "INFO:dedupe.canopy_index:Removing stop word so\n",
      "INFO:dedupe.canopy_index:Removing stop word mi\n",
      "INFO:dedupe.canopy_index:Removing stop word  t\n",
      "INFO:dedupe.canopy_index:Removing stop word au\n",
      "INFO:dedupe.canopy_index:Removing stop word ux\n",
      "INFO:dedupe.canopy_index:Removing stop word à \n",
      "INFO:dedupe.canopy_index:Removing stop word mp\n",
      "INFO:dedupe.canopy_index:Removing stop word  g\n",
      "INFO:dedupe.canopy_index:Removing stop word ts\n",
      "INFO:dedupe.canopy_index:Removing stop word n \n",
      "INFO:dedupe.canopy_index:Removing stop word av\n",
      "INFO:dedupe.canopy_index:Removing stop word ed\n",
      "INFO:dedupe.canopy_index:Removing stop word ig\n",
      "INFO:dedupe.canopy_index:Removing stop word ng\n",
      "INFO:dedupe.canopy_index:Removing stop word gi\n",
      "INFO:dedupe.canopy_index:Removing stop word é \n",
      "INFO:dedupe.canopy_index:Removing stop word no\n",
      "INFO:dedupe.canopy_index:Removing stop word rt\n",
      "INFO:dedupe.blocking:10000, 3.0307652 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering complete. 3387 clusters found. It took 64.455 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Cell 2.2: Run the model, check out the results.\n",
    "\n",
    "# The main parameter you can change here is recall_weight. If you increase this number, the model will\n",
    "# care more about finding possible matches (recall). If you reduce the number, the model will care more about\n",
    "# getting the matches right when it does find them (precision).\n",
    "deduper, matches = run_deduper(deduper, data_frame, settings_file, training_file, recall_weight = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Inspecting the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing clustered data to illegal_books_clustered.csv...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Cell 3.1. Add cluster data back to original data frame and save to csv\n",
    "_ = save_clusters(matches, data_frame, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>UUID</th>\n",
       "      <th>super_book_code</th>\n",
       "      <th>full_book_title</th>\n",
       "      <th>author_code</th>\n",
       "      <th>author_name</th>\n",
       "      <th>stated_publication_places</th>\n",
       "      <th>stated_publication_years</th>\n",
       "      <th>cluster</th>\n",
       "      <th>confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5928</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>spbk0001648</td>\n",
       "      <td>Memoires Turcs  où hist. galante de deux Turcs...</td>\n",
       "      <td>au0000038</td>\n",
       "      <td>Aucour, Claude Godard [d']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1357.0</td>\n",
       "      <td>0.745271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6354</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>spbk0001648</td>\n",
       "      <td>Memoires Turcs ou histoire galante de deux Tur...</td>\n",
       "      <td>au0000038</td>\n",
       "      <td>Aucour, Claude Godard [d']</td>\n",
       "      <td>Francfort</td>\n",
       "      <td>1765</td>\n",
       "      <td>1357.0</td>\n",
       "      <td>0.735028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6853</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>spbk0001648</td>\n",
       "      <td>Memoires Turcs</td>\n",
       "      <td>au0000038</td>\n",
       "      <td>Aucour, Claude Godard [d']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1357.0</td>\n",
       "      <td>0.832770</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID UUID super_book_code  \\\n",
       "5928 NaN  NaN     spbk0001648   \n",
       "6354 NaN  NaN     spbk0001648   \n",
       "6853 NaN  NaN     spbk0001648   \n",
       "\n",
       "                                        full_book_title author_code  \\\n",
       "5928  Memoires Turcs  où hist. galante de deux Turcs...   au0000038   \n",
       "6354  Memoires Turcs ou histoire galante de deux Tur...   au0000038   \n",
       "6853                                     Memoires Turcs   au0000038   \n",
       "\n",
       "                     author_name stated_publication_places  \\\n",
       "5928  Aucour, Claude Godard [d']                       NaN   \n",
       "6354  Aucour, Claude Godard [d']                 Francfort   \n",
       "6853  Aucour, Claude Godard [d']                       NaN   \n",
       "\n",
       "     stated_publication_years  cluster  confidence  \n",
       "5928                      NaN   1357.0    0.745271  \n",
       "6354                     1765   1357.0    0.735028  \n",
       "6853                      NaN   1357.0    0.832770  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 3.2. Sanity check. How good are the model's assignments?\n",
    "# Run this cell a few times to look at different random clusters\n",
    "data_frame[data_frame['cluster'] == random.randint(0, len(matches) + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "532 illegal books have been given super_book_codes, of 1921 that lack them.\n"
     ]
    }
   ],
   "source": [
    "# Cell 3.3. How much time have we saved? How many illegal books have been assigned a super_book_code?\n",
    "assigned = data_frame[\n",
    "    pd.notnull(data_frame['cluster']) & # which books have been assigned a cluster?\n",
    "    pd.notnull(data_frame['UUID']) & # only illegal books have UUIDs\n",
    "    pd.isna(data_frame['super_book_code']) # only interested in books that didn't already have super_book_codes\n",
    "].shape[0]\n",
    "\n",
    "total = data_frame_clustered[\n",
    "    pd.notnull(data_frame['UUID']) &\n",
    "    pd.isna(data_frame['super_book_code'])\n",
    "].shape[0]\n",
    "\n",
    "print(f\"{assigned} illegal books have been given super_book_codes, of {total} that lack them.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can consider the accuracy of the model more accurately by seeing how often it clustered books with different super_book_codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Of the 3387 clusters found by dedupe, 717 contain multiple superbooks.\n"
     ]
    }
   ],
   "source": [
    "# Cell 3.4. Inspecting the super_book_codes in all the clusters.\n",
    "multi_groups = (data_frame.groupby(by=\"cluster\")['super_book_code'] # group into clusters, inspect 'super_book_code'\n",
    "               .nunique() # count how many unqiue 'super_book_codes' are in the cluster\n",
    "               .where(lambda x: x > 1) # only keep clusters with more than one 'super_book_code'\n",
    "               .dropna()) # drop the NaNs created by .where()\n",
    "\n",
    "print(f\"Of the {len(matches)} clusters found by dedupe, {len(multi_groups)} contain multiple superbooks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>UUID</th>\n",
       "      <th>super_book_code</th>\n",
       "      <th>full_book_title</th>\n",
       "      <th>author_code</th>\n",
       "      <th>author_name</th>\n",
       "      <th>stated_publication_places</th>\n",
       "      <th>stated_publication_years</th>\n",
       "      <th>cluster</th>\n",
       "      <th>confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2034</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>spbk0000541</td>\n",
       "      <td>Dictionnaire de l'Académie Françoise</td>\n",
       "      <td>au0001311</td>\n",
       "      <td>Académie Française</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>412.0</td>\n",
       "      <td>0.792762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6350</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>spbk0000541</td>\n",
       "      <td>Dictionnaire de l'academie françoise nouvelle ...</td>\n",
       "      <td>au0001311</td>\n",
       "      <td>Académie Française</td>\n",
       "      <td>Lyon</td>\n",
       "      <td>1776</td>\n",
       "      <td>412.0</td>\n",
       "      <td>0.879901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10575</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>zspbk0010615</td>\n",
       "      <td>Dictionnaire de l’academie françoise.  Nouvell...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Lyon</td>\n",
       "      <td>1776</td>\n",
       "      <td>412.0</td>\n",
       "      <td>0.876813</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ID UUID super_book_code  \\\n",
       "2034  NaN  NaN     spbk0000541   \n",
       "6350  NaN  NaN     spbk0000541   \n",
       "10575 NaN  NaN    zspbk0010615   \n",
       "\n",
       "                                         full_book_title author_code  \\\n",
       "2034                Dictionnaire de l'Académie Françoise   au0001311   \n",
       "6350   Dictionnaire de l'academie françoise nouvelle ...   au0001311   \n",
       "10575  Dictionnaire de l’academie françoise.  Nouvell...         NaN   \n",
       "\n",
       "              author_name stated_publication_places stated_publication_years  \\\n",
       "2034   Académie Française                       NaN                      NaN   \n",
       "6350   Académie Française                      Lyon                     1776   \n",
       "10575                 NaN                      Lyon                     1776   \n",
       "\n",
       "       cluster  confidence  \n",
       "2034     412.0    0.792762  \n",
       "6350     412.0    0.879901  \n",
       "10575    412.0    0.876813  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 3.5. Which books has Dedupe confounded?\n",
    "# Pick one of the groups\n",
    "rand_multi = int(random.choice(multi_groups.index.tolist()))\n",
    "\n",
    "# Inspect it\n",
    "data_frame[data_frame['cluster'] == rand_multi]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Of the 3387 clusters found by dedupe, 102 contain multiple illegal books.\n"
     ]
    }
   ],
   "source": [
    "# Are there any clusters where it has put more than one illegal book?\n",
    "multi_illegal = (data_frame[pd.notnull(data_frame['UUID'])]\n",
    "                 .groupby(by = 'cluster')['UUID']\n",
    "                 .nunique()\n",
    "                 .where(lambda x: x > 1)\n",
    "                 .dropna())\n",
    "\n",
    "print(f\"Of the {len(matches)} clusters found by dedupe, {len(multi_illegal)} contain multiple illegal books.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>UUID</th>\n",
       "      <th>super_book_code</th>\n",
       "      <th>full_book_title</th>\n",
       "      <th>author_code</th>\n",
       "      <th>author_name</th>\n",
       "      <th>stated_publication_places</th>\n",
       "      <th>stated_publication_years</th>\n",
       "      <th>cluster</th>\n",
       "      <th>confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1657</th>\n",
       "      <td>1744.0</td>\n",
       "      <td>9f316381-78db-41cd-9c43-e40bf2701f9a</td>\n",
       "      <td>NaN</td>\n",
       "      <td>L'Espion anglois</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pidansat de Mairobert, Mathieu Franc?ois</td>\n",
       "      <td>London</td>\n",
       "      <td>1785</td>\n",
       "      <td>290.0</td>\n",
       "      <td>0.869183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1658</th>\n",
       "      <td>1745.0</td>\n",
       "      <td>4e7e8462-92dc-4ce9-a203-e0d56e836e51</td>\n",
       "      <td>NaN</td>\n",
       "      <td>L'Espion anglois</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pidansat de Mairobert, Mathieu Franc?ois</td>\n",
       "      <td>London</td>\n",
       "      <td>1785</td>\n",
       "      <td>290.0</td>\n",
       "      <td>0.869183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4023</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>spbk0001429</td>\n",
       "      <td>Lettres originales de Madame la comtesse du Ba...</td>\n",
       "      <td>au0000705</td>\n",
       "      <td>Pidansat de Mairobert, Mathieu-François</td>\n",
       "      <td>London</td>\n",
       "      <td>1779</td>\n",
       "      <td>290.0</td>\n",
       "      <td>0.758484</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID                                  UUID super_book_code  \\\n",
       "1657  1744.0  9f316381-78db-41cd-9c43-e40bf2701f9a             NaN   \n",
       "1658  1745.0  4e7e8462-92dc-4ce9-a203-e0d56e836e51             NaN   \n",
       "4023     NaN                                   NaN     spbk0001429   \n",
       "\n",
       "                                        full_book_title author_code  \\\n",
       "1657                                   L'Espion anglois         NaN   \n",
       "1658                                   L'Espion anglois         NaN   \n",
       "4023  Lettres originales de Madame la comtesse du Ba...   au0000705   \n",
       "\n",
       "                                   author_name stated_publication_places  \\\n",
       "1657  Pidansat de Mairobert, Mathieu Franc?ois                    London   \n",
       "1658  Pidansat de Mairobert, Mathieu Franc?ois                    London   \n",
       "4023   Pidansat de Mairobert, Mathieu-François                    London   \n",
       "\n",
       "     stated_publication_years  cluster  confidence  \n",
       "1657                     1785    290.0    0.869183  \n",
       "1658                     1785    290.0    0.869183  \n",
       "4023                     1779    290.0    0.758484  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examine some of these ones:\n",
    "# Pick one of the groups\n",
    "rand_multi = int(random.choice(multi_illegal.index.tolist()))\n",
    "\n",
    "# Inspect it\n",
    "data_frame[data_frame['cluster'] == rand_multi]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Using the results\n",
    "\n",
    "The question is: how to use the results? The most obvious course seems to be to be this: go through all the illegal books, and keep the most confident result that the model has put out. Then we can manually go over them, and if they are okay, update the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy of the data frame\n",
    "out_data = data_frame.copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now go through each cluster, get the highest confidence super_book_code and assign it to the illegal book\n",
    "(out_data.groupby(col = \"cluster\")\n",
    "    .)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Dedupe appears to do a good job in linking the illegal books to super books that are already in the dataset. The question is whether the ~1600 books the algorithm could not cluster are new books that aren't already in our dataset, or if the algorithm has low recall.\n",
    "\n",
    "Manual investigation will be necessary to see if the ~1600 super books are already in the database.\n",
    "\n",
    "It may also be possible to tune the model further, by\n",
    "1. giving it more training data, or\n",
    "2. increasing the `recall_weight` so that the model cares more about finding possible matches than being accurate when it does find them.\n",
    "\n",
    "**Addendum (12/11/2018):** Simon reckons that finding `super_book_codes` for only ~300 of the illegal books is unsurprising. His hunch is that the authorities were quite good at extinguishing banned titles most of the time in *ancien r&eacute;gime* France.\n",
    "\n",
    "**Addendum (19/11/2018):** I tried generating a whole lot of training data, using the super book codes to find matching and non-matching pairs. Feeding this to the model using the `Dedupe.markPairs()` method, the model's recall jumped considerably, and it has found 600+ matches with the illegal books. Now to check that data and upload it..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
