########################################################################################################
#
# MMF Record Linkage Project
#
# Project: Mapping Print, Charting Enlightenment
#
# Script: A look at the imprint data
#
# Authors: Michael Falk
#
# Date: 21/1/19
#
# There is a lot of geospatial data stored in free-text fields in the FBTEE datasets.
# In particular, in many of the datasets we have the imprint of the books, which describes
# where they were published. If we could extract this data and geotag it, we could systematically
# compare book's purported places of publication with the places they are bought, sold or otherwise
# acted upon. For instance, we can see from the customs register where books supposedly published in 
# 'Paris' or 'London' are actually arriving from.
#
# How hard would it be to extract places from the imprint data?
#
########################################################################################################

# Get data
source("init.R")

editions <- manuscripts %>%
  dbSendQuery("SELECT * FROM manuscript_books_editions ") %>%
  fetch(n = Inf) %>%
  as.tibble()

place <- manuscripts %>%
  dbSendQuery("SELECT * FROM places ") %>%
  fetch(n = Inf) %>%
  as.tibble()

# What does the imprint data look like?
editions %>%
  select(stated_publication_places, actual_publication_places) %>%
  sample_n(100) %>%
  print(n = Inf)

##### SECTION 1: EXTRACT NORMALISED PLACE NAMES FROM EDITION TABLE ####

# It looks to be pretty consistent:
# Older books: a missing stated place is indicated by 'n. pl.' a missing actual place is indicated by '?'
# Newer books: missing places are indicated by empty strings

# What is the maximum number of places?
num_stated <- editions %>%
  pull(stated_publication_places) %>%
  # str_match_all returns a list of character matrices
  str_match_all("(;)") %>%
  # the number of semicolons found = number of columns -1, so total individual places = number of columns
  sapply(FUN = length) %>%
  max()
  
num_actual <- editions %>%
  pull(actual_publication_places) %>%
  # str_match_all returns a list of character matrices
  str_match_all("(;)") %>%
  # the number of semicolons found = number of columns -1, so total individual places = number of columns
  sapply(FUN = length) %>%
  max()

# So it seems there are up to eight stated publication places, and up to four actual ones, for each edition.

# Which books have more than three of either?
editions %>%
  filter(sapply(str_match_all(stated_publication_places, "(;)"), length) > 3) %>%
  select(full_book_title, stated_publication_places, actual_publication_places) %>%
  print(n = Inf)

editions %>%
  filter(sapply(str_match_all(actual_publication_places, "(;)"), length) > 3) %>%
  select(full_book_title, stated_publication_places, actual_publication_places) %>%
  print(n = Inf)

# Create new tables for many-to-many join:
ostensibly_published_in <- editions %>%
  select(book_code, stated_publication_places) %>%
  # Filter out missing data
  filter(!str_detect(stated_publication_places, 'n\\.pl\\.'),
         nchar(stated_publication_places) > 0) %>%
  # Split place data on semicolons into eight columns (NB: how num_stated was calculated above)
  separate(stated_publication_places, into = paste0("place", (1:num_stated)), sep = "; ") %>%
  # Move all place names into single column
  gather(key = "place_number", value = "place_name", -book_code) %>%
  # Drop superfluous column generated by 'gather'
  select(-place_number) %>%
  # Drop empty values (most books have only one place, so the other 7 columsn would have 'NA')
  drop_na()

actually_published_in <- editions %>%
  select(book_code, actual_publication_places) %>%
  # Filter out missing data
  filter(!str_detect(actual_publication_places, '\\?'),
         nchar(actual_publication_places) > 0) %>%
  # Split place data on semicolons into eight columns (NB: how num_actual was calculated above)
  separate(actual_publication_places, into = paste0("place", (1:num_actual)), sep = "; ") %>%
  # Move all place names into single column
  gather(key = "place_number", value = "place_name", -book_code) %>%
  # Drop superfluous column generated by 'gather'
  select(-place_number) %>%
  # Drop empty values (most books have only one place, so the other 7 columns would have 'NA')
  drop_na()

# Summary data on number of publication places
ostensibly_published_in %>%
  group_by(book_code) %>%
  summarise(n = n()) %>%
  group_by(n) %>%
  summarise(freq = n())

actually_published_in %>%
  group_by(book_code) %>%
  summarise(n = n()) %>%
  group_by(n) %>%
  summarise(freq = n())


##### SECTION 2: LINK EXTRACTED PLACES TO PLACE CODES WHERE POSSIBLE #####

# For each place name associated with a book, we're going to find the best match we can from the place data.

# First preprocess the text
# On analysing the first go at this, I noticed that places with 'la' in the name were frustrating the algorithm.
# Strip out those lalala's!

str_norm <- function(x) {
  out <- x %>%
    tolower() %>%
    str_replace("(, )*\\bl[ae]s*\\b|(, )*\\bthe\\b", "") %>%
    str_trim()
  
  return(out)
}

place %<>%
  mutate(canonical = str_norm(name))

ostensibly_published_in %<>%
  mutate(norm = str_norm(place_name))

actually_published_in %<>%
  mutate(norm = str_norm(place_name))

# Then compute the distance matrix
ost <- stringdistmatrix(ostensibly_published_in$norm, place$canonical)
act <- stringdistmatrix(actually_published_in$norm, place$canonical)

# The closest matches have the lowest scores
library(Rfast)
ost_min <- Rfast::rowMins(ost)
ost_min_val <- Rfast::rowMins(ost, value = T)
act_min <- Rfast::rowMins(act)
act_min_val <- Rfast::rowMins(act, value = T)

# Now bind the results to the join tables
ostensibly_published_in %<>%
  mutate(place_code = place$place_code[ost_min],
         place_canonical_name = place$name[ost_min],
         place_canonical_norm = place$canonical[ost_min],
         confidence = ost_min_val) %>%
  # Normalise the confidence
  mutate(confidence = 1 - (confidence / max(nchar(norm), nchar(place_canonical_norm))))

actually_published_in %<>%
  mutate(place_code = place$place_code[act_min],
         place_canonical_name = place$name[act_min],
         place_canonical_norm = place$canonical[act_min],
         confidence = act_min_val) %>%
  # Normalise the confidence
  mutate(confidence = 1 - (confidence / max(nchar(norm), nchar(place_canonical_norm))))


##### SECTION 3: EXPORT DATA TO SPREADSHEET FOR BURROWS TO EXAMINE #####

actually_published_in %>%
  select(-place_canonical_norm, -norm) %>%
  left_join(select(editions, book_code, full_book_title), by = "book_code") %>%
  write_csv("actual_place_match.csv")

ostensibly_published_in %>%
  select(-place_canonical_norm, -norm) %>%
  left_join(select(editions, book_code, full_book_title), by = "book_code") %>%
  write_csv("ostensible_place_match.csv")
